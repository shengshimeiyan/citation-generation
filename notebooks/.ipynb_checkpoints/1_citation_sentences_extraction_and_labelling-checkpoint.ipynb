{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_sentence</th>\n",
       "      <th>manuscript_id</th>\n",
       "      <th>cited_id</th>\n",
       "      <th>background_prob</th>\n",
       "      <th>method_prob</th>\n",
       "      <th>result_prob</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In addition, we here report comparable changes...</td>\n",
       "      <td>8281087</td>\n",
       "      <td>15228934</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.942871</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In addition, we here report comparable changes...</td>\n",
       "      <td>8281087</td>\n",
       "      <td>20547173</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.942871</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In addition, we here report comparable changes...</td>\n",
       "      <td>8281087</td>\n",
       "      <td>15671550</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.942871</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Such a discrepancy may likely be due to dose-o...</td>\n",
       "      <td>8281923</td>\n",
       "      <td>6704669</td>\n",
       "      <td>0.45166</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.527902</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Similar to earlier observations (42) , the inf...</td>\n",
       "      <td>11155963</td>\n",
       "      <td>4009171</td>\n",
       "      <td>0.223237</td>\n",
       "      <td>0.105094</td>\n",
       "      <td>0.671669</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   citation_sentence manuscript_id  cited_id  \\\n",
       "0  In addition, we here report comparable changes...       8281087  15228934   \n",
       "1  In addition, we here report comparable changes...       8281087  20547173   \n",
       "2  In addition, we here report comparable changes...       8281087  15671550   \n",
       "3  Such a discrepancy may likely be due to dose-o...       8281923   6704669   \n",
       "4  Similar to earlier observations (42) , the inf...      11155963   4009171   \n",
       "\n",
       "  background_prob method_prob result_prob  intent  \n",
       "0        0.046504    0.010625    0.942871  result  \n",
       "1        0.046504    0.010625    0.942871  result  \n",
       "2        0.046504    0.010625    0.942871  result  \n",
       "3         0.45166    0.020438    0.527902  result  \n",
       "4        0.223237    0.105094    0.671669  result  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = {}\n",
    "with open(\"../data/citation_generation/result_comps.jsonl\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        d[i] = json.loads(line)\n",
    "        \n",
    "result_comps = pd.DataFrame.from_dict(d).T\n",
    "display(result_comps.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_sentence</th>\n",
       "      <th>manuscript_id</th>\n",
       "      <th>cited_id</th>\n",
       "      <th>background_prob</th>\n",
       "      <th>method_prob</th>\n",
       "      <th>result_prob</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Such a discrepancy may likely be due to dose-o...</td>\n",
       "      <td>8281923</td>\n",
       "      <td>6704669</td>\n",
       "      <td>0.45166</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>0.527902</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Similar to earlier observations (42) , the inf...</td>\n",
       "      <td>11155963</td>\n",
       "      <td>4009171</td>\n",
       "      <td>0.223237</td>\n",
       "      <td>0.105094</td>\n",
       "      <td>0.671669</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In a cohort study in Korea (20) , the highest ...</td>\n",
       "      <td>11158470</td>\n",
       "      <td>15153392</td>\n",
       "      <td>0.417643</td>\n",
       "      <td>0.049526</td>\n",
       "      <td>0.532832</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This finding can be explained by the fact that...</td>\n",
       "      <td>11159272</td>\n",
       "      <td>14840523</td>\n",
       "      <td>0.01713</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.977294</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The distribution of flow velocity and WSS was ...</td>\n",
       "      <td>11159272</td>\n",
       "      <td>13786003</td>\n",
       "      <td>0.016776</td>\n",
       "      <td>0.019489</td>\n",
       "      <td>0.963735</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   citation_sentence manuscript_id  cited_id  \\\n",
       "0  Such a discrepancy may likely be due to dose-o...       8281923   6704669   \n",
       "1  Similar to earlier observations (42) , the inf...      11155963   4009171   \n",
       "2  In a cohort study in Korea (20) , the highest ...      11158470  15153392   \n",
       "3  This finding can be explained by the fact that...      11159272  14840523   \n",
       "4  The distribution of flow velocity and WSS was ...      11159272  13786003   \n",
       "\n",
       "  background_prob method_prob result_prob  intent  \n",
       "0         0.45166    0.020438    0.527902  result  \n",
       "1        0.223237    0.105094    0.671669  result  \n",
       "2        0.417643    0.049526    0.532832  result  \n",
       "3         0.01713    0.005576    0.977294  result  \n",
       "4        0.016776    0.019489    0.963735  result  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how many citation sentences only cite one paper?\n",
    "unique = result_comps.drop_duplicates(subset=[\"citation_sentence\"], keep=False).reset_index(drop=True)\n",
    "display(unique.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such a discrepancy may likely be due to dose-or species-specific differences since an acute administration of 3.3 mg/kg MDMA did not stimulate locomotion in mice (Scearce-Levie et al, 1999) , but did in rats ( Bankson and Cunningham, 2002) .\n",
      "\n",
      "{'section': 'INTRODUCTION', 'text': 'It is well known that serotonin (5-HT) regulates feeding behavior, as shown by the potent anorectic properties of 5-HT releasers, such as fenfluramine. Pharmacological studies combined with knockout strategies have clearly demonstrated that among the 15 5-HT receptor subtypes, the 5-HT receptors are key elements regulating food intake in mammals. The 5-HT 1B , 5-HT 2A , and 5-HT 2C receptor agonists produce hypophagia (Bendotti and Samanin, 1987 ; Kennett and Curzon, 1988 ; Schechter and Simansky, 1988; Macor et al, 1990; Aulakh et al, 1994; Halford and Blundell, 1996 ; Lee and Simansky, 1997; Lucas et al, 1998; Vickers et al, 2001; Lee et al, 2004) . Conversely, the inactivation of 5-HT 2C receptors alters fenfluramineinduced anorexia (Neill and Cooper, 1989; Vickers et al, 1999 Vickers et al, , 2001 Heisler et al, 2002) . 5-HT 2C receptor-null mice, known for being obese (Tecott et al, 1995) , display reduced sensitivity to fenfluramine (Vickers et al, 1999 (Vickers et al, , 2001 ). The possible roles of 5-HT 1B receptors in d-fenfluramine-induced anorexia are more complex. Acute treatment with GR127935, a 5-HT 1B receptor antagonist, or with cyanopindolol, a 5-HT 1A/1B receptor antagonist, does not modify d-fenfluramine-induced anorexia in starved rats (Vickers et al, 2001) . In contrast, 5-HT 1B receptor-null mice are less sensitive to d-fenfluramine than wild-type mice (Lucas et al, 1998) .', 'cite_spans': []}\n",
      "\n",
      "{'section': 'INTRODUCTION', 'text': \"Among drugs that increase synaptic levels of 5-HT, antidepressants are known clearly to modify food intake, whereas the action of some others such as 3,4-N-methylenedioxymethamphetamine (MDMA or 'ecstasy') have not been investigated. This is surprising since MDMA is a widely used recreational drug. MDMA is defined as a 'substratetype 5-HT releaser' (Rothman and Baumann, 2002) and is an amphetamine-like stimulant. MDMA (10 mg/kg) increases the levels of 5-HT, but can also increase dopamine (DA) levels, especially at higher doses (Colado et al, 2004) .\", 'cite_spans': []}\n",
      "\n",
      "{'section': 'INTRODUCTION', 'text': 'The primary goals of this study were to investigate whether MDMA (10 mg/kg) could counteract starvationinduced eating and whether the 5-HT receptors were involved in this effect.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'MATERIALS AND METHODS', 'text': '', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Animals', 'text': 'All experiments were performed on male adult wild-type and 5-HT 1B receptor-null mice on a 129/Sv genetic background (see Phillips et al, 1999 for a description of the exact genetic background of the mice), aged 4-6 months. The 5-HT 1B receptor-null mice were generated as described previously (Saudou et al, 1994) . Both the wild-type and null mice were all obtained from heterozygous breeding at the transgenic animal facility of the UPR CNRS 5203 in Montpellier (France). The genotype of each mouse was identified using the PCR technique. For the 5-HT 2C receptor experimental paradigms, wild-type mice on a 129/Sv (Iffa Credo, France) were used at 4-6 months old. Mice were housed five per cage with food and water available ad libitum and maintained in a temperature-controlled environment on a 12-h light/dark cycle with light onset at 06:00. The experiments were carried out in accordance with the Guide for Care and Use of Laboratory Animals established by the National Institutes of Health of the United States of America.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Procedures', 'text': 'In summary, three different sets of experiments were performed using the feeding paradigm described in detail below and previously used by Lucas and co-workers (1998) . First, the wild-type and 5-HT 1B receptor-null mice were treated with a single intraperitoneal (i.p.) injection of saline, MDMA, or 5-HT 1B receptor agonist. Second, another group of wild-type mice was similarly injected with NaCl, MDMA, or 5-HT 1B receptor antagonist GR127935 either alone or in combination with MDMA. Third, NaCl, MDMA, or 5-HT 2C receptor antagonist RS102221 alone or combined with MDMA was administered in the same manner to a naïve group of wild-type mice. Finally, four additional groups of wild-type mice were treated with NaCl, MDMA, or RS102221 either alone or coadministered with MDMA and tested in open field chambers (Activity section), as described previously (Compan et al, 2003 . In each experimental paradigm, animals were treated with an equivalent volume (3.3 ml/kg) of vehicle via the same route.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Feeding Paradigm Tests', 'text': 'The feeding procedure followed was exactly the same as the one used by Lucas and co-workers for fenfluramine (1998) . At 3 days before the experiments, 5-HT 1B receptor-null and wild-type mice were housed singly with ad libitum access to classic food (pellet form, 16.5% crude proteins, 3.6% crude fat, crude fiber 4.6%, ash 5.2%) and tap water. Animals were then food deprived for 24 h with water given ad libitum (09:00 to 21:00), as previously used in mice (Lucas et al, 1998) and rats (23 h, Vickers et al, 2001) . Tap water consumption was not measured. On the testing day, classic food was reintroduced into the trough 10 min after an acute i.p. injection of one of the treatments described above. Food was briefly removed (o20 s) and weighed at 1 and 3 h following the initial reintroduction. First, we tested the effects of MDMA (10 mg/kg) in mice of both genotypes. To make sure that our results did not depend on different experimentalists or laboratory conditions, other groups of mice of both genotypes were also treated with the 5-HT 1A/1B receptor agonist RU24969 (5 mg/kg), as used by Lucas and co-workers (1998) . In parallel, we assessed for the first time the effects of the selective 5-HT 1B receptors agonist CGS12066A (0.5, 1 or 2 mg/kg) in wild-type and null mice. Second, wild-type mice received a 3, 5, or 10 mg/kg dose of the 5-HT 1B/1D receptor antagonist GR127935 coadministered with or without MDMA (10 mg/kg). Third, a series of experiments were conducted following the injection of a 1 or 2 mg/kg dose of the selective 5-HT 2C receptor antagonist RS102221 that was combined or not to MDMA (10 mg/kg) in wild-type mice.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Feeding Paradigm Tests', 'text': 'In each experiment, animals received an i.p. injection of sterile 0.9% NaCl. Food intake and body weight were measured daily using a Sartorius CP32S balance (1 mg precision), which automatically provides the mean body weight of 10 values for each animal in movement. ', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Pharmacological Treatments', 'text': '', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Activity', 'text': 'Open-field test. Naïve male mice were tested for 3 h following NaCl, MDMA (10 mg/kg), RS102221 (2 mg/kg dose), or MDMA combined with RS102221. Testing was conducted between 09:00 and 17:00. The open-field test environment is a square chamber with an inside area that measures: 43.2 Â 43.2 Â 30.5 cm the mice made a slow, side to side or lateral movement), licking, and grooming were determined using the following scoring scale: 1 ¼ 1-4 times, 2 ¼ 5-10 times, 3 ¼ 11-20 times, and 4 ¼ more than 20 times. In addition, grooming and licking were counted for each animal. Such a study was not performed for 5-HT 1B receptor-related experimental paradigms because we have previously performed detailed analyses indicating that MDMA (10 mg/kg)-induced locomotion is reduced in 5-HT 1B receptor-null mice (ScearceLevie et al, 1999; Compan et al, 2003) .', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Data Analysis', 'text': 'Data were analyzed using Statview 5 (Abacus concept, Berkeley, USA). For each set of experiments (Procedure section), a repeated measures ANOVA was systematically performed on the data, which were obtained in multiple sessions over time. Genotype and treatment were used as independent variables. Food intake or activity parameters were used as dependent variables. If significant effects of genotype or treatment, or a genotype and treatment interaction, over time or not, were found, the independent variables were split for a two-(genotype and treatment) or one-way ANOVA (genotype or treatment) analysis. Following a significant one-way ANOVA, we used the Scheffé Ftest for multiple comparisons, with a probability of 0.01 and 0.05 defined as a significant difference. In order to simplify the presentation in the Results section, only the results from the separate ANOVAs conducted on each time period are described.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'RESULTS', 'text': '', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Similar Baseline Food Intake and Body Weight of 5-HT 1B Receptor-Null and Wild-Type Mice Born from Couples of Heterozygous Animals', 'text': 'Two earlier studies have reported that male 5-HT 1B receptor-null mice are overweight and consume a higher amount of food than wild-type mice (Brunner et al, 1999; Bouwknecht et al, 2001) . However, the validity of the results was a matter of debate because wild-type and null mice were obtained from independent lines (Bouwknecht et al, 2001) . Therefore, we used homozygote offspring from heterozygote breeding pairs (Animal section) in order to avoid indirect effects of parental care behavioral responses, as discussed by Bouwknecht et al (2001) . No difference in either baseline food intake or body weight was detected between wild-type and 5-HT 1B receptor-null mice for the habituation period or following a 24 h period of food deprivation (data not shown), as reported previously (Lucas et al, 1998 MDMA produced a profound reduction in food intake for 1 h in starved wild-type mice, as compared with NaCltreated wild-type mice (À77%, Figure 1a) . However, after this initial reduction in food intake, we observed that the amount of food consumed over the next 2 h (between 60 and 180 min) was greater in MDMA-treated mice than in NaClinjected wild-type mice ( þ 47%, Figure 1b) . Therefore, the drive to eat induced by starvation was suppressed during the first hour by MDMA. Compared to saline-treated starved mice, MDMA-treated starved mice first displayed a period of hypophagia for 1 h and then a second period of hyperphagia for the next 2 h.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Similar Baseline Food Intake and Body Weight of 5-HT 1B Receptor-Null and Wild-Type Mice Born from Couples of Heterozygous Animals', 'text': 'As with MDMA, the 5-HT 1B receptor agonist CGS12066A (2 mg/kg, but not 0.5 or 1 mg/kg) reduced food consumption during the first hour following treatment (À65%, Figure 1a ). However, in contrast to MDMA, CGS12066A-induced hypophagia was prolonged over 3 h (Figure 1b) . Similar results were obtained using the 5-HT 1A/1B receptor agonist RU24969 (5 mg/kg, Figure 1a and b), as reported previously (Lucas et al, 1998) . Thus, 5-HT 1B receptor agonists produced a more sustained hypophagia than does MDMA (Figure 1b) . Figure 1 Biphasic feeding responses to MDMA in starved wild-type and 5-HT 1B receptor-null mice contrasted to a sustained hypophagia of 5-HT 1B receptor agonists. Data are means7SEM total food intake for groups of wild-type ( þ / þ ) and 5-HT 1B receptor-null (À/À) mice treated with NaCl (n ¼ 22-17), MDMA (10 mg/kg, n ¼ 11-9), 5-HT 1A/1B receptor agonist RU24969 (5 mg/kg, n ¼ 9-7), or 5-HT 1B receptor agonist CGS12066A (2 mg/kg, n ¼ 11-7). Food intake was significantly different between mice of both genotypes (F 1,85 ¼ 6.2, po0.05) and changed after the treatment used for 1 h (F 3,85 ¼ 23.4, po0.0001) and 60-180 min periods (F 3,85 ¼ 13.5, po0.0001). Treatments that differ significantly from saline in wild-type mice are marked ( Since both MDMA and 5-HT 1B receptor agonists reduced food intake in starved mice, we thought that 5-HT 1B receptors could be involved in MDMA-delayed eating. However, we did not verify this hypothesis since, in 5-HT 1B receptor-null mice, MDMA induced a biphasic effect on feeding behavior similar to the one observed in wild-type mice: a hypophagic effect over 1 h (À58%, Figure 1a) followed by a hyperphagic effect (Figure 1b) . As expected, note that, no significant effect of CGS12066A (2 mg/kg) or RU24969 (5 mg/kg) injected alone was observed in 5-HT 1B receptor-null mice (Figure 1a and b) .', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Similar Baseline Food Intake and Body Weight of 5-HT 1B Receptor-Null and Wild-Type Mice Born from Couples of Heterozygous Animals', 'text': 'Biphasic Feeding Responses to MDMA were Maintained when 5-HT 1B Receptors were Blocked by the Antagonist GR127935', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Similar Baseline Food Intake and Body Weight of 5-HT 1B Receptor-Null and Wild-Type Mice Born from Couples of Heterozygous Animals', 'text': 'To circumvent any compensatory mechanisms, which may occur in mice lacking 5-HT 1B receptors from birth, we tested whether an acute injection of the specific 5-HT 1B/1D receptor antagonist, GR127935 (10 mg/kg), combined with MDMA may disrupt the effects of MDMA on feeding.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'Similar Baseline Food Intake and Body Weight of 5-HT 1B Receptor-Null and Wild-Type Mice Born from Couples of Heterozygous Animals', 'text': 'Results indicate that in starved wild-type mice, MDMA plus GR127935 did not modify the biphasic change in food intake provoked by MDMA (Figure 2a and b) . In addition, no significant effect on food intake was detected in mice treated with 3 (not shown) or 10 mg/kg of GR127935, as compared to saline-injected wild-type animals ( Figure 2a and b).', 'cite_spans': []}\n",
      "\n",
      "{'section': '5-HT 2C', 'text': 'Receptor Antagonist RS102221 Highly Reduced MDMA-Induced Hypophagia, but not Its HyperphagicDelayed Response 5-HT 2C receptor-null mice exhibit an increase in food intake (Tecott et al, 1995) , unrelated to peripheral change (Nonogaki et al, 1998) . Pharmacological studies have reported that the 5-HT 2C receptor antagonist SB242084 reduced the anorectic effects of both m-chlorophenylpiperazine, a mixed 5-HT 2A,2B,2C receptor agonist (Kennett et al, 1997) , and fenfluramine (Vickers et al, 2001 ). However, SB242084 alone did not induce hyperphagia, nor did it mimic other behavioral parameters of 5-HT 2C receptor-null mice (Kennett et al, 1997) . Therefore, we chose to use another selective 5-HT 2C receptor antagonist, RS102221, because its chronic administration mimics the feeding phenotype of 5-HT 2C -null mice (Bonhaus et al, 1997) .', 'cite_spans': []}\n",
      "\n",
      "{'section': '5-HT 2C', 'text': 'In this new series of experiments, the treatments induced significant changes in food intake over time (1 h: F 3,36 ¼ 12.5, po0.0001; 60-180 min: F 3,36 ¼ 8.3, po0.001). Again, we observed a biphasic change in food intake following MDMA injection in starved wild-type animals (Figure 3a and b) . RS102221 (2 mg/kg) alone did not change the feeding response for the first period (0-60 min, Figure 3a) . Interestingly, if RS102221 (2 mg/kg) was coadministered with MDMA, the hypophagic effect of MDMA observed during this period was greatly reduced compared to the one observed after injection of MDMA alone (Figure 3a) . In contrast, for the second period (60-180 min), the hyperphagia detected after MDMA was not reduced after coinjection of RS102221 and MDMA (Figure 3b ). Note that RS102221 injected alone induced hyperphagia during this period (Figure 3b ).', 'cite_spans': []}\n",
      "\n",
      "{'section': 'MDMA did not Induce Hyperlocomotion when Coinjected with RS102221', 'text': 'There is an obvious correlation between horizontal locomotion and feeding responses. To our knowledge, the possible involvement of 5-HT 2C receptors in MDMA-induced Figure 2 Biphasic feeding responses to MDMA was maintained even when the drug is coadministered with the 5-HT 1B/1D receptor antagonist GR127935. Data are means7SEM total food intake measured 1 h (a) and between 1 and 3 h (b) in wild-type mice treated with NaCl (n ¼ 10), MDMA (10 mg/kg, n ¼ 9), 5-HT 1B/1D receptor antagonist GR127935 (10 mg/kg, n ¼ 8), or MDMA/GR127935 (n ¼ 10) in starved wild-type mice. Significant differences between any treatment and NaCl effect are noted ( }}} po0.0001, Suppression of disorders after MDMA/RS102221 treatment G Conductier et al hyperactivity has never been tested in mice. Our results indicate that MDMA significantly increased the traveled path length, as compared to saline-injected animals ( Figure 4a ). In contrast, a combined treatment of MDMA plus RS102221 failed to induce any change in horizontal activity (Figure 4a) . Similarly, the total traveled path length was markedly increased after the administration of MDMA, as compared to control mice ( þ 592%, Figure 4b ). This MDMA-induced activity was highly reduced by the coinjection of RS102221 ( þ 226%, po0.01) (Figure 4a and b) . Note that RS102221 alone had no effect on locomotor activity (Figure 4a and b) . The effects of MDMA alone on horizontal activity were significantly greater than for mice treated with MDMA plus RS102221 (MDMA vs MDMA þ RS102221: þ 226%, po0.01), or RS102221 (MDMA vs RS102221: þ 435%, Figure 4c ). In summary, MDMA stimulated locomotion in mice when injected alone, but not when coadministered with RS102221. Stereotyped behaviors could also compete with both horizontal locomotion and feeding changes after MDMA. The absence of a reduction in the hypophagic response to MDMA when 5-HT 2C receptors were inactivated may be explained by the fact that mice exhibited less stereotyped behaviors after cotreatment with MDMA plus RS102221. To investigate such a possibility, we assessed the degree of certain stereotyped behaviors. Wild-type mice treated with MDMA, RS102221, or MDMA plus RS102221 exhibited no significant difference in the number of head weaving, grooming, or licking behaviors, as compared to control animals (not shown). In contrast, both MDMA and MDMA plus RS102221 treatments induced marked increases in the number of flat body and stretch-attend postures observed, as compared to control mice, but only for the initial 30 min following drug injection (Figure 4d ). No significant difference was detected after use of any treatment over the next 30 min (Figure 4e) . Therefore, the recorded stereotyped behaviors may not compete with feeding or motor responses.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'DISCUSSION', 'text': \"There are many studies reporting solid evidence that MDMA administration increases 5-HT steady-state content in several brain structures of rats and mice ( McKenna and Peroutka, 1990; Colado et al, 2004) . Consequently, scientists focused on the effects of the drug on serotonergic system functions, even though MDMA has varying potencies in increasing the synaptic levels of other biogenic amines, such as DA (McKenna and Peroutka, 1990; Colado et al, 2004) . One dose of 10 mg/kg MDMA (i.p., 3 h), identical to that used under our experimental conditions, was found to increase 5-HT turnover in mouse brain (Steele et al, 1989) . Few studies indicate that MDMA exerts an effect on food consumption in humans (anorectic or bulimic effects) (Shulgin, 1986; Rochester and Kirchner, 1999; Schifano, 2000) , as would be expected from its 'substrate-type 5-HT releaser' activity. Furthermore, only one study reports its hypophagic effect in rats (Frith et al, 1987) . Curiously, no attempt has been made to investigate the effect of MDMA on feeding behavior in mice and the possible implication of 5-HT receptors. Our results indicate that MDMA induced an initial hypophagia in spite of starvation, suggesting that the drug induced a pharmacological effect sufficient to disrupt the physiological drive in mice to eat. However, this hypophagic effect was transient (1 h) since over the following period (60-180 min) MDMA-treated mice exhibited hyperphagia compared to saline-treated mice. Therefore, the kinetics of the feeding response to MDMA was biphasic. We first thought that 5-HT 1B receptors could be implicated in the effects of MDMA in starved mice. However, several points argue against this hypothesis. Firstly, we found that 5-HT 1B receptor agonists (CGS12066A and RU24969) effectively reduced food consumption, but they did so differently to MDMA. Their effects were monophasic with a hypophagic response prolonged over the 180 min period of investigation. Secondly, there was no difference observed in the biphasic feeding response to MDMA on feeding behavior in starved 5-HT 1B receptor-null mice vs wild-type animals. Thirdly, the 5-HT 1B/1D receptor antagonist GR127935 did not modify the biphasic effect of MDMA on feeding behavior in starved mice. This result is consistent with that obtained by Vickers et al (2001) , which showed that GR127935 as well as the 5-HT 1A/1B receptor antagonist cyanopindolol did not reduce fenfluramineinduced anorexia in starved rats. However, 5-HT 1B receptor-null mice are insensitive to fenfluramine-induced anorexia. These apparently contradictory results may be explained either by a developmental problem or, as suggested, by a reduction of 5-HT 2C receptor function in 5-HT 1B receptor-null mice (Clifton et al, 2003) . Compensatory effects of other 5-HT receptors may occur in 5-HT 1B receptor-null mice. For example, a compensatory elevation in 5-HT 4 receptor function is likely because the 5-HT 4 receptor antagonist RS39604 reduced MDMA-induced hypophagia . It remains to be determined why the 5-HT 1B receptor-null mice are sensitive to MDMA-induced anorexia but not to fenfluramineinduced anorexia. Fenfluramine and MDMA have already been reported to induce different behavioral changes. MDMA administration provoked an increase in locomotion (Rempel et al, 1993) and temperature ( Malberg and Seiden, 1998) , whereas fenfluramine causes hypolocomotion (Bankson and Cunningham, 2001 ) and hypothermia (Cryan et al, 2000) .\", 'cite_spans': [{'start': 3305, 'end': 3325, 'text': '(Rempel et al, 1993)', 'cite_id': '7182899'}, {'start': 3344, 'end': 3369, 'text': 'Malberg and Seiden, 1998)', 'cite_id': '13984991'}]}\n",
      "\n",
      "{'section': 'DISCUSSION', 'text': 'The implication of 5-HT 2C receptors in MDMA-induced hypophagia is clear. The present results indicate that MDMA-induced hypophagia was greatly reduced if MDMA was coadministered with the 5-HT 2C receptor antagonist RS102221. Injection of RS102221 also blocked the motor responses to MDMA, indicating that the dose used (2 mg/ kg) is effective (see Bonhaus et al, 1997 for discussion). Inactivation of 5-HT 2C receptors also has been reported to mediate reduced food intake following 5-HT releasers, such as fluoxetine (Lightowler et al, 1996) , d-fenfluramine (Neill and Cooper, 1989; Vickers et al, 2001) , and norfenfluramine (Vickers et al, 2001) . Thus, MDMA acts via inducing an increase in 5-HT levels in the synaptic cleft followed by the activation of 5-HT 2C receptors. A direct action of MDMA on 5-HT 2C receptors cannot be excluded since MDMA does bind to 5-HT 2 receptors with high affinity (0.6-6 mM) (Lyon et al, 1986; Battaglia and De Souza, 1989) .', 'cite_spans': []}\n",
      "\n",
      "{'section': 'DISCUSSION', 'text': 'MDMA is a drug of abuse that elicits a dramatic increase in locomotion (Geyer and Callaway, 1994) , suggesting the possibility that MDMA-induced hypophagia could be due to hyperlocomotion. However, this seems unlikely at least for one reason: MDMA (10 mg/kg)-induced hyperlocomotion was reduced after treatment with GR127935 and in 5-HT 1B receptor-null mice (Scearce-Levie et al, 1999; Compan et al, 2003) , whereas the hypophagic effects of MDMA were not. Likewise, several studies have excluded the possibility that nonspecific behavioral changes, such as hyperlocomotion and even sedation, may account for the hypophagic responses to 5-HT 1B , 5-HT 2A , 5-HT 2B , or 5-HT 2C receptor agonists ( Kitchener and Dourish, 1994 ; Lee and Simansky, 1997; Hewitt et al, 2002; Clifton et al, 2000 Clifton et al, , 2003 . The fact that 5-HT 2C receptors may contribute to MDMA-induced hyperlocomotion has been questioned in one study using rats (Bankson and Cunningham, 2002) . This investigation reported that SB206553, a 5-HT 2C receptor antagonist, enhanced the motor effects of MDMA in the rat (Bankson and Cunningham, 2002) . Such a discrepancy may likely be due to dose-or species-specific differences since an acute administration of 3.3 mg/kg MDMA did not stimulate locomotion in mice (Scearce-Levie et al, 1999) , but did in rats ( Bankson and Cunningham, 2002) . This difference may depend on the fact that a 10 mg/kg dose of MDMA induces a marked increase in the DA release in rats, while only a modest rise is obtained at the same dose in mice (see Yamamoto et al, 1995; Colado et al, 2004) . Moreover, the use of different antagonists may also lead to varied results. It has been shown recently that SB206553 is not a neutral antagonist, but an inverse agonist of 5-HT 2C receptors. This compound elevated the extracellular DA levels in the nucleus accumbens of rats by a significantly greater magnitude than did the 5-HT 2C receptor antagonist SB242084 (De Deurwaerdère et al, 2004) . Consequently, the influence of serotonergic systems, in part by 5-HT 2C receptors, appears to be dependent on the degree of elevation of DA concentration, as extensively debated by Bankson and Cunningham, in studies using rats (2001) .', 'cite_spans': [{'start': 940, 'end': 970, 'text': '(Bankson and Cunningham, 2002)', 'cite_id': '6704669'}, {'start': 1093, 'end': 1123, 'text': '(Bankson and Cunningham, 2002)', 'cite_id': '6704669'}, {'start': 1336, 'end': 1365, 'text': 'Bankson and Cunningham, 2002)', 'cite_id': '6704669'}]}\n",
      "\n",
      "{'section': 'DISCUSSION', 'text': 'Our results suggest that 5-HT 2C receptor transmission may underlie the initial hypophagic effect of MDMA, but that is likely not responsible for the following period of hyperphagia. Indeed, the delayed hyperphagic effect of MDMA (period 60-180 min) was not modified by the coinjection of RS102221 and MDMA. Nevertheless, this is possible that hyperphagia could be mediated via one or several other neuronal systems such as peptidergic and/or dopaminergic systems.', 'cite_spans': []}\n",
      "\n",
      "{'section': 'DISCUSSION', 'text': 'These findings present the first evidence that a pharmacological inactivation of 5-HT 2C receptors using RS102221 overcomes both MDMA-induced feeding and motor disorders in mice.', 'cite_spans': []}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(unique.loc[0,\"citation_sentence\"])\n",
    "print()\n",
    "\n",
    "with open(\"../data/s2orc/s2orc_result_subset.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        p = json.loads(line)\n",
    "        if p[\"paper_id\"] != unique.loc[0,\"manuscript_id\"]:\n",
    "            continue\n",
    "        for para in p[\"body_text\"]:\n",
    "            print(para)\n",
    "            print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique.to_json(\"./unique_result_comps.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Extracting citation sentencesduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktParameters, PunktSentenceTokenizer\n",
    "\n",
    "# tokenizer should not split at abbrieviations\n",
    "punkt_params = PunktParameters()\n",
    "punkt_params.abbrev_types = set([\"i.e\", \"e.g\", \"etc\", \"al\", \"fig\", \"figs\", \n",
    "                                 \"ref\", \"refs\", \"p\", \"c\", \"s\"]) \n",
    "\n",
    "# initialise sentence tokenizer\n",
    "punkt_sent_tokenizer = PunktSentenceTokenizer(punkt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:27<00:00, 3655.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract correct citation sentences\n",
    "citation_sentences = []\n",
    "\n",
    "## for each paper, \n",
    "for i in tqdm(range(len(s2orc))):\n",
    "    manuscript_id = s2orc.loc[i, \"paper_id\"]\n",
    "    full_text = s2orc.loc[i, \"body_text\"]\n",
    "\n",
    "    for paragraph in full_text:\n",
    "        ## skip paragraphs that are not in \"discussion\" section \n",
    "        section_name = paragraph[\"section\"].lower()\n",
    "        if \"discuss\" not in section_name and \"conclu\" not in section_name:\n",
    "            continue \n",
    "            \n",
    "        ## also skip paragraphs with no citation sentences\n",
    "        if not paragraph[\"cite_spans\"]: \n",
    "            continue\n",
    "\n",
    "        ## tokenize paragraph into sentences \n",
    "        paragraph_text = paragraph[\"text\"]\n",
    "        endpoints = list(punkt_sent_tokenizer.span_tokenize(paragraph_text))\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        ## for each citation marker, \n",
    "        for cite_span in paragraph[\"cite_spans\"]:\n",
    "            cite_id = cite_span[\"cite_id\"]\n",
    "            cite_text = cite_span[\"text\"]\n",
    "            start, end = cite_span[\"start\"], cite_span[\"end\"]\n",
    "\n",
    "            ## extract the sentence containing the citation marker\n",
    "            a, b = endpoints[j]\n",
    "            while start >= b:\n",
    "                j += 1\n",
    "                a, b = endpoints[j]\n",
    "\n",
    "            ## if citation marker is textual or sentence begins with words, \n",
    "            ## assume extracted sentence is true citation sentence\n",
    "            ## else, take previous sentence instead\n",
    "            textual = re.search('[a-zA-Z]', cite_text) \n",
    "            if not (textual or re.search(\"[a-zA-Z]\", paragraph_text[a:start])): \n",
    "                a, b = endpoints[j-1]\n",
    "\n",
    "            citation_sentence = paragraph_text[a:b]\n",
    "            citation_sentences.append((citation_sentence, manuscript_id, cite_id))\n",
    "\n",
    "# convert to pd.DataFrame\n",
    "citation_sentences = pd.DataFrame(\n",
    "    citation_sentences, \n",
    "    columns=[\"citation_sentence\", \"manuscript_id\", \"cited_id\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_sentence</th>\n",
       "      <th>manuscript_id</th>\n",
       "      <th>cited_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Specifically, we generalized the distributiona...</td>\n",
       "      <td>18980380</td>\n",
       "      <td>7229756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Further studies are needed to understand wheth...</td>\n",
       "      <td>18981358</td>\n",
       "      <td>21704892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The absence of changes in the perceived positi...</td>\n",
       "      <td>18981358</td>\n",
       "      <td>11263174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In addition to enzymes, some ligands, such as ...</td>\n",
       "      <td>18982781</td>\n",
       "      <td>25252408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Furthermore, the number of estimated deaths (5...</td>\n",
       "      <td>18985891</td>\n",
       "      <td>205233560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189865</th>\n",
       "      <td>In addition, abnormal inflammatory input may i...</td>\n",
       "      <td>11685058</td>\n",
       "      <td>3548631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189866</th>\n",
       "      <td>Current and proposed clinical guidelines allow...</td>\n",
       "      <td>11685058</td>\n",
       "      <td>18494690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189867</th>\n",
       "      <td>Many investigators are dedicated to unveil the...</td>\n",
       "      <td>11685696</td>\n",
       "      <td>8854921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189868</th>\n",
       "      <td>Obesity is an independent risk factor for OSA ...</td>\n",
       "      <td>11685696</td>\n",
       "      <td>26741728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189869</th>\n",
       "      <td>Adipose tissue deposited around the pharynx an...</td>\n",
       "      <td>11685696</td>\n",
       "      <td>38710134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189870 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        citation_sentence manuscript_id  \\\n",
       "0       Specifically, we generalized the distributiona...      18980380   \n",
       "1       Further studies are needed to understand wheth...      18981358   \n",
       "2       The absence of changes in the perceived positi...      18981358   \n",
       "3       In addition to enzymes, some ligands, such as ...      18982781   \n",
       "4       Furthermore, the number of estimated deaths (5...      18985891   \n",
       "...                                                   ...           ...   \n",
       "189865  In addition, abnormal inflammatory input may i...      11685058   \n",
       "189866  Current and proposed clinical guidelines allow...      11685058   \n",
       "189867  Many investigators are dedicated to unveil the...      11685696   \n",
       "189868  Obesity is an independent risk factor for OSA ...      11685696   \n",
       "189869  Adipose tissue deposited around the pharynx an...      11685696   \n",
       "\n",
       "         cited_id  \n",
       "0         7229756  \n",
       "1        21704892  \n",
       "2        11263174  \n",
       "3        25252408  \n",
       "4       205233560  \n",
       "...           ...  \n",
       "189865    3548631  \n",
       "189866   18494690  \n",
       "189867    8854921  \n",
       "189868   26741728  \n",
       "189869   38710134  \n",
       "\n",
       "[189870 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outpath = \"../misc/citation_sentences.jsonl\"\n",
    "if os.path.exists(outpath):\n",
    "    d = load_data(outpath)\n",
    "    citation_sentences = pd.DataFrame.from_dict(d).T\n",
    "else: \n",
    "    # save as json\n",
    "    citation_sentences.to_json(outpath, orient=\"records\", lines=True)\n",
    "    \n",
    "citation_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Filter out citation sentences with wrong intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5934/5934 [00:15<00:00, 386.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess citation sentences\n",
    "dataloader = DataLoader(citation_sentences[\"citation_sentence\"], \n",
    "                        sampler=SequentialSampler(citation_sentences[\"citation_sentence\"]), \n",
    "                        batch_size=BATCH_SIZE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_WEIGHTS)\n",
    "\n",
    "# get input_ids, attention_masks encodings\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for batch in tqdm(dataloader):\n",
    "    b_input_ids, b_attn_mask = tokenizer(batch, \n",
    "                                         max_length=MAX_LEN, \n",
    "                                         padding=\"max_length\",\n",
    "                                         truncation=True,  \n",
    "                                         add_special_tokens=True, \n",
    "                                         return_token_type_ids=False).values()\n",
    "\n",
    "    input_ids.extend(b_input_ids)\n",
    "    attention_masks.extend(b_attn_mask)\n",
    "    \n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for model input\n",
    "encodings = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "# process 1000 sentences each time\n",
    "model_input = DataLoader(encodings[:100], \n",
    "                         sampler=SequentialSampler(encodings[:100]), \n",
    "                         batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 7.77 GiB total capacity; 3.71 GiB already allocated; 17.50 MiB free; 3.86 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-526555084f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# forward pass through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/citation_intent_classification.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         outputs = self.BERT(input_ids=input_ids, \n\u001b[0;32m---> 77\u001b[0;31m                             attention_mask=attention_mask)\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only take CLS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 )\n\u001b[1;32m    490\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 7.77 GiB total capacity; 3.71 GiB already allocated; 17.50 MiB free; 3.86 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# pass citation sentences through intent classifier \n",
    "# output index_to_prob (dict)\n",
    "# - index (int): index of a row whose citation sentence has intent \"result-comparison\"\n",
    "# - prob (float): computed probability of \"result-comparison\" intent\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "i, m = 0, 0\n",
    "index_to_prob = {}\n",
    "for batch in tqdm(model_input): \n",
    "        \n",
    "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # forward pass through model\n",
    "        logits = model(b_input_ids, b_attn_mask)\n",
    "        \n",
    "    \n",
    "    # compute probabilities of each label\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "    # get indices where \"result-comparison\" has highest probability\n",
    "    ind = np.where(np.argmax(probs, axis=1) == 2)[0]\n",
    "\n",
    "    # create dict of (index, prob) pairs and update index_to_prob\n",
    "    # index = ind + batch_size * i = row number in original input data\n",
    "    d = dict(zip(ind + batch_size * i, probs[ind,2]))\n",
    "    index_to_prob.update(d)\n",
    "        \n",
    "    i += 1\n",
    "    m += len(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_sentence</th>\n",
       "      <th>manuscript_id</th>\n",
       "      <th>cited_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These ␤␣␤␤␣ folds have also been detected in o...</td>\n",
       "      <td>25056663</td>\n",
       "      <td>28215109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KH domains consist of a three-stranded ␤ sheet...</td>\n",
       "      <td>25056663</td>\n",
       "      <td>28215109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However, the typical GXXG motif required for D...</td>\n",
       "      <td>25056663</td>\n",
       "      <td>28215109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(24 -26) We also confirm the welldocumented re...</td>\n",
       "      <td>25593307</td>\n",
       "      <td>11387986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The N-terminal part of S. cerevisiae contains ...</td>\n",
       "      <td>25593541</td>\n",
       "      <td>26887717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240484</th>\n",
       "      <td>In this regard, it was also noticeable that th...</td>\n",
       "      <td>8359616</td>\n",
       "      <td>17485379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240485</th>\n",
       "      <td>In this regard, it was also noticeable that th...</td>\n",
       "      <td>8359616</td>\n",
       "      <td>14591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240486</th>\n",
       "      <td>The other eight SP-proteins identified in Sus ...</td>\n",
       "      <td>8359616</td>\n",
       "      <td>24241995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240487</th>\n",
       "      <td>Noticeable, deoxyribonuclease -2-alpha, an aci...</td>\n",
       "      <td>8359616</td>\n",
       "      <td>39723204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240488</th>\n",
       "      <td>This does not seem be the case, as sperm from ...</td>\n",
       "      <td>8359616</td>\n",
       "      <td>23559424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240489 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        citation_sentence manuscript_id  \\\n",
       "0       These ␤␣␤␤␣ folds have also been detected in o...      25056663   \n",
       "1       KH domains consist of a three-stranded ␤ sheet...      25056663   \n",
       "2       However, the typical GXXG motif required for D...      25056663   \n",
       "3       (24 -26) We also confirm the welldocumented re...      25593307   \n",
       "4       The N-terminal part of S. cerevisiae contains ...      25593541   \n",
       "...                                                   ...           ...   \n",
       "240484  In this regard, it was also noticeable that th...       8359616   \n",
       "240485  In this regard, it was also noticeable that th...       8359616   \n",
       "240486  The other eight SP-proteins identified in Sus ...       8359616   \n",
       "240487  Noticeable, deoxyribonuclease -2-alpha, an aci...       8359616   \n",
       "240488  This does not seem be the case, as sperm from ...       8359616   \n",
       "\n",
       "        cited_id  \n",
       "0       28215109  \n",
       "1       28215109  \n",
       "2       28215109  \n",
       "3       11387986  \n",
       "4       26887717  \n",
       "...          ...  \n",
       "240484  17485379  \n",
       "240485     14591  \n",
       "240486  24241995  \n",
       "240487  39723204  \n",
       "240488  23559424  \n",
       "\n",
       "[240489 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = load_data(\"./citation_sentences_subset.jsonl\")\n",
    "citation_sentences_0_2 = pd.DataFrame.from_dict(d).T\n",
    "citation_sentences_0_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_sentence</th>\n",
       "      <th>manuscript_id</th>\n",
       "      <th>cited_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A study in 5 government and 12 private hospita...</td>\n",
       "      <td>25214135</td>\n",
       "      <td>44347243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Moreover, these health disparities are not exp...</td>\n",
       "      <td>25217127</td>\n",
       "      <td>41231576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Another important area of future study is the ...</td>\n",
       "      <td>25217127</td>\n",
       "      <td>19474685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>However, due to limitations in the computation...</td>\n",
       "      <td>21610508</td>\n",
       "      <td>18523563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to published anecdotal descriptions ...</td>\n",
       "      <td>21617327</td>\n",
       "      <td>67777532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145782</th>\n",
       "      <td>However, the mesh, as a foreign body itself, u...</td>\n",
       "      <td>22603883</td>\n",
       "      <td>25101686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145783</th>\n",
       "      <td>Interestingly, previous research with postinst...</td>\n",
       "      <td>7530033</td>\n",
       "      <td>30892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145784</th>\n",
       "      <td>We and others have previously demonstrated tha...</td>\n",
       "      <td>7530965</td>\n",
       "      <td>39845408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145785</th>\n",
       "      <td>We and others have previously demonstrated tha...</td>\n",
       "      <td>7530965</td>\n",
       "      <td>10791690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145786</th>\n",
       "      <td>This is the first evidence to our knowledge of...</td>\n",
       "      <td>7530965</td>\n",
       "      <td>9881561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10145787 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citation_sentence manuscript_id  \\\n",
       "0         A study in 5 government and 12 private hospita...      25214135   \n",
       "1         Moreover, these health disparities are not exp...      25217127   \n",
       "2         Another important area of future study is the ...      25217127   \n",
       "3         However, due to limitations in the computation...      21610508   \n",
       "4         According to published anecdotal descriptions ...      21617327   \n",
       "...                                                     ...           ...   \n",
       "10145782  However, the mesh, as a foreign body itself, u...      22603883   \n",
       "10145783  Interestingly, previous research with postinst...       7530033   \n",
       "10145784  We and others have previously demonstrated tha...       7530965   \n",
       "10145785  We and others have previously demonstrated tha...       7530965   \n",
       "10145786  This is the first evidence to our knowledge of...       7530965   \n",
       "\n",
       "          cited_id  \n",
       "0         44347243  \n",
       "1         41231576  \n",
       "2         19474685  \n",
       "3         18523563  \n",
       "4         67777532  \n",
       "...            ...  \n",
       "10145782  25101686  \n",
       "10145783     30892  \n",
       "10145784  39845408  \n",
       "10145785  10791690  \n",
       "10145786   9881561  \n",
       "\n",
       "[10145787 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = load_data(\"./citation_sentences_3_84.jsonl\")\n",
    "citation_sentences_3_84 = pd.DataFrame.from_dict(d).T\n",
    "citation_sentences_3_84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
