{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "from tqdm import tqdm \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from citation_intent_classification import CiteBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1 GPU(s) available.\n",
      "Device name: GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# check for GPU\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"There is {torch.cuda.device_count()} GPU(s) available.\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. S2ORC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jessica/data/s2orc/s2orc_fullbody_subset_100K.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b1115d440a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/jessica/data/s2orc/s2orc_fullbody_subset_100K.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0ms2orc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time taken: {round(time.time()-s,4)} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b1115d440a79>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jessica/data/s2orc/s2orc_fullbody_subset_100K.jsonl'"
     ]
    }
   ],
   "source": [
    "def load_data(filepath):\n",
    "    d = {}\n",
    "    with open(filepath) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            d[i] = json.loads(line)\n",
    "    return d\n",
    "\n",
    "s = time.time()\n",
    "filepath = \"/home/jessica/data/s2orc/s2orc_fullbody_subset_100K.jsonl\"\n",
    "d = load_data(filepath)\n",
    "s2orc = pd.DataFrame.from_dict(d).T\n",
    "print(f\"Time taken: {round(time.time()-s,4)} seconds\")\n",
    "\n",
    "display(s2orc.head())\n",
    "\n",
    "# drop unnecessary columns\n",
    "s2orc = s2orc[[\"paper_id\", \"body_text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Extracting citation sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer should not split at abbrieviations\n",
    "punkt_params = PunktParameters()\n",
    "punkt_params.abbrev_types = set([\"i.e\", \"e.g\", \"etc\", \"al\", \"fig\", \"figs\", \n",
    "                                 \"ref\", \"refs\", \"p\", \"c\", \"s\"]) \n",
    "\n",
    "# initialise sentence tokenizer\n",
    "punkt_sent_tokenizer = PunktSentenceTokenizer(punkt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recently, we found that ethanolamine (EA) or ethylenediamine (ED)-functionalized poly(glycidyl methacrylate) (PGMA), namely PGEA or PGED, could be used as effective gene carriers. 15, 16 They possess good gene transfection properties. To further improve the performance of PGMA-based gene carriers, several strategies have been applied such as polysaccharide introduction and target molecule binding. 16, 17 Owing to the dynamically unable ability of supramolecular polymers, the application of supramolecular chemistry for gene delivery has been a hot research topic in the biomedical field. 18, 19 The construction of supramolecular polycations via host-guest interaction is a popular strategy for high-efficiency gene delivery systems. 20 In particular, cyclodextrins (CDs) and their derivatives have been widely utilized for constructing supramolecular gene delivery systems, mainly because of their superior biocompatibility. [21] [22] [23] With the host-guest interaction strategy, we successfully prepared one PGEA-based supramolecular delivery system by tying multiple β-cyclodextrin (β-CD)cored star PGEA polymers to an adamantine-modified linear PGEA backbone. 24 Such PGEA supramolecules markedly increased transfection efficiencies. Further improvements in functionality and the development of new preparation strategies for PGMA-based supramolecular vectors would benefit the construction of better gene delivery systems.\n",
      "\n",
      "[22]: [21] [22] [23] With the host-guest interaction strategy, we successfully prepared one PGEA-based supramolecular delivery system by tying multiple β-cyclodextrin (β-CD)cored star PGEA polymers to an adamantine-modified linear PGEA backbone.\n",
      "\n",
      "[23]: [21] [22] [23] With the host-guest interaction strategy, we successfully prepared one PGEA-based supramolecular delivery system by tying multiple β-cyclodextrin (β-CD)cored star PGEA polymers to an adamantine-modified linear PGEA backbone.\n"
     ]
    }
   ],
   "source": [
    "# (try to) extract citation sentences from example\n",
    "for paragraph in s2orc.loc[0, \"body_text\"]:\n",
    "    cite_spans = paragraph[\"cite_spans\"]\n",
    "    \n",
    "    # ignore paragraphs without citations within S2ORC\n",
    "    if not cite_spans: continue \n",
    "        \n",
    "    paragraph_text = paragraph[\"text\"]\n",
    "    print(paragraph_text)\n",
    "        \n",
    "    # tokenize the paragraph into sentences\n",
    "    endpoints = list(punkt_sent_tokenizer.span_tokenize(paragraph_text))\n",
    "    \n",
    "    j = 0\n",
    "    # for each citation marker, \n",
    "    for cite_span in cite_spans:\n",
    "        cite_text = cite_span[\"text\"]\n",
    "        start, end = cite_span[\"start\"], cite_span[\"end\"]\n",
    "\n",
    "        # get the sentence that contains cite_text\n",
    "        a, b = endpoints[j]\n",
    "        while start >= b:\n",
    "            j += 1\n",
    "            a, b = endpoints[j]\n",
    "            \n",
    "        print(f\"\\n{cite_text}: {paragraph_text[a:b]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recently, we found that ethanolamine (EA) or ethylenediamine (ED)-functionalized poly(glycidyl methacrylate) (PGMA), namely PGEA or PGED, could be used as effective gene carriers. 15, 16 They possess good gene transfection properties. To further improve the performance of PGMA-based gene carriers, several strategies have been applied such as polysaccharide introduction and target molecule binding. 16, 17 Owing to the dynamically unable ability of supramolecular polymers, the application of supramolecular chemistry for gene delivery has been a hot research topic in the biomedical field. 18, 19 The construction of supramolecular polycations via host-guest interaction is a popular strategy for high-efficiency gene delivery systems. 20 In particular, cyclodextrins (CDs) and their derivatives have been widely utilized for constructing supramolecular gene delivery systems, mainly because of their superior biocompatibility. [21] [22] [23] With the host-guest interaction strategy, we successfully prepared one PGEA-based supramolecular delivery system by tying multiple β-cyclodextrin (β-CD)cored star PGEA polymers to an adamantine-modified linear PGEA backbone. 24 Such PGEA supramolecules markedly increased transfection efficiencies. Further improvements in functionality and the development of new preparation strategies for PGMA-based supramolecular vectors would benefit the construction of better gene delivery systems.\n",
      "\n",
      "[22] - 20 In particular, cyclodextrins (CDs) and their derivatives have been widely utilized for constructing supramolecular gene delivery systems, mainly because of their superior biocompatibility.\n",
      "\n",
      "[23] - 20 In particular, cyclodextrins (CDs) and their derivatives have been widely utilized for constructing supramolecular gene delivery systems, mainly because of their superior biocompatibility.\n"
     ]
    }
   ],
   "source": [
    "# extract based on citation marker type \n",
    "# if marker is textual, e.g. \"Devlin et al., 2018\", \"CS93\", \"(Ehrman, 2020)\" \n",
    "#     then extract sentence containing the marker\n",
    "# \n",
    "# else marker is non-textual, e.g. \"21\", \"[1]\"\n",
    "#     if marker is at the start of the sentence containing it, e.g. \"[21] [22] With the host-guest ...\"\n",
    "#         then marker actually belongs to the previous sentence\n",
    "#         extract previous sentence\n",
    "# \n",
    "#     else there are words before the marker, e.g. \"It has been shown in [21] that ...\"\n",
    "#         then marker belongs to this sentence\n",
    "#         extract sentence containing the marker\n",
    "\n",
    "for paragraph in s2orc.loc[0, \"body_text\"]:\n",
    "    # skip paragraphs with no citations\n",
    "    if not paragraph[\"cite_spans\"]: continue \n",
    "        \n",
    "    paragraph_text = paragraph[\"text\"]\n",
    "    print(paragraph_text)\n",
    "        \n",
    "    # tokenize the paragraph into sentences\n",
    "    endpoints = list(punkt_sent_tokenizer.span_tokenize(paragraph_text))\n",
    "    \n",
    "    j = 0\n",
    "    cite_spans = paragraph[\"cite_spans\"]\n",
    "    # for each citation marker, \n",
    "    for cite_span in cite_spans:\n",
    "        cite_text = cite_span[\"text\"]\n",
    "        start, end = cite_span[\"start\"], cite_span[\"end\"]\n",
    "        \n",
    "        # get the sentence that contains cite_text\n",
    "        a, b = endpoints[j]\n",
    "        while start >= b:\n",
    "            j += 1\n",
    "            a, b = endpoints[j]\n",
    "        \n",
    "        # get the correct citation sentence\n",
    "        textual = re.search(\"[a-zA-Z]\", cite_text)\n",
    "        if not (textual or re.search(\"[a-zA-Z]\", paragraph_text[a:start])): \n",
    "            a, b = endpoints[j-1]\n",
    "\n",
    "        print(f\"\\n{cite_text} - {paragraph_text[a:b]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:27<00:00, 3655.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract correct citation sentences\n",
    "citation_sentences = []\n",
    "\n",
    "## for each paper, \n",
    "for i in tqdm(range(len(s2orc))):\n",
    "    manuscript_id = s2orc.loc[i, \"paper_id\"]\n",
    "    full_text = s2orc.loc[i, \"body_text\"]\n",
    "\n",
    "    for paragraph in full_text:\n",
    "        ## skip paragraphs that are not in \"discussion\" section \n",
    "        section_name = paragraph[\"section\"].lower()\n",
    "        if \"discuss\" not in section_name and \"conclu\" not in section_name:\n",
    "            continue \n",
    "            \n",
    "        ## also skip paragraphs with no citation sentences\n",
    "        if not paragraph[\"cite_spans\"]: \n",
    "            continue\n",
    "\n",
    "        ## tokenize paragraph into sentences \n",
    "        paragraph_text = paragraph[\"text\"]\n",
    "        endpoints = list(punkt_sent_tokenizer.span_tokenize(paragraph_text))\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        ## for each citation marker, \n",
    "        for cite_span in paragraph[\"cite_spans\"]:\n",
    "            cite_id = cite_span[\"cite_id\"]\n",
    "            cite_text = cite_span[\"text\"]\n",
    "            start, end = cite_span[\"start\"], cite_span[\"end\"]\n",
    "\n",
    "            ## extract the sentence containing the citation marker\n",
    "            a, b = endpoints[j]\n",
    "            while start >= b:\n",
    "                j += 1\n",
    "                a, b = endpoints[j]\n",
    "\n",
    "            ## if citation marker is textual or sentence begins with words, \n",
    "            ## assume extracted sentence is true citation sentence\n",
    "            ## else, take previous sentence instead\n",
    "            textual = re.search('[a-zA-Z]', cite_text) \n",
    "            if not (textual or re.search(\"[a-zA-Z]\", paragraph_text[a:start])): \n",
    "                a, b = endpoints[j-1]\n",
    "\n",
    "            citation_sentence = paragraph_text[a:b]\n",
    "            citation_sentences.append((citation_sentence, manuscript_id, cite_id))\n",
    "\n",
    "# convert to pd.DataFrame\n",
    "citation_sentences = pd.DataFrame(\n",
    "    citation_sentences, \n",
    "    columns=[\"citation_sentence\", \"manuscript_id\", \"cited_id\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citation_sentence</th>\n",
       "      <th>manuscript_id</th>\n",
       "      <th>cited_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Specifically, we generalized the distributiona...</td>\n",
       "      <td>18980380</td>\n",
       "      <td>7229756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Further studies are needed to understand wheth...</td>\n",
       "      <td>18981358</td>\n",
       "      <td>21704892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The absence of changes in the perceived positi...</td>\n",
       "      <td>18981358</td>\n",
       "      <td>11263174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In addition to enzymes, some ligands, such as ...</td>\n",
       "      <td>18982781</td>\n",
       "      <td>25252408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Furthermore, the number of estimated deaths (5...</td>\n",
       "      <td>18985891</td>\n",
       "      <td>205233560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189865</th>\n",
       "      <td>In addition, abnormal inflammatory input may i...</td>\n",
       "      <td>11685058</td>\n",
       "      <td>3548631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189866</th>\n",
       "      <td>Current and proposed clinical guidelines allow...</td>\n",
       "      <td>11685058</td>\n",
       "      <td>18494690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189867</th>\n",
       "      <td>Many investigators are dedicated to unveil the...</td>\n",
       "      <td>11685696</td>\n",
       "      <td>8854921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189868</th>\n",
       "      <td>Obesity is an independent risk factor for OSA ...</td>\n",
       "      <td>11685696</td>\n",
       "      <td>26741728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189869</th>\n",
       "      <td>Adipose tissue deposited around the pharynx an...</td>\n",
       "      <td>11685696</td>\n",
       "      <td>38710134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189870 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        citation_sentence manuscript_id  \\\n",
       "0       Specifically, we generalized the distributiona...      18980380   \n",
       "1       Further studies are needed to understand wheth...      18981358   \n",
       "2       The absence of changes in the perceived positi...      18981358   \n",
       "3       In addition to enzymes, some ligands, such as ...      18982781   \n",
       "4       Furthermore, the number of estimated deaths (5...      18985891   \n",
       "...                                                   ...           ...   \n",
       "189865  In addition, abnormal inflammatory input may i...      11685058   \n",
       "189866  Current and proposed clinical guidelines allow...      11685058   \n",
       "189867  Many investigators are dedicated to unveil the...      11685696   \n",
       "189868  Obesity is an independent risk factor for OSA ...      11685696   \n",
       "189869  Adipose tissue deposited around the pharynx an...      11685696   \n",
       "\n",
       "         cited_id  \n",
       "0         7229756  \n",
       "1        21704892  \n",
       "2        11263174  \n",
       "3        25252408  \n",
       "4       205233560  \n",
       "...           ...  \n",
       "189865    3548631  \n",
       "189866   18494690  \n",
       "189867    8854921  \n",
       "189868   26741728  \n",
       "189869   38710134  \n",
       "\n",
       "[189870 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outpath = \"../misc/citation_sentences.jsonl\"\n",
    "if os.path.exists(outpath):\n",
    "    d = load_data(outpath)\n",
    "    citation_sentences = pd.DataFrame.from_dict(d).T\n",
    "else: \n",
    "    # save as json\n",
    "    citation_sentences.to_json(outpath, orient=\"records\", lines=True)\n",
    "    \n",
    "citation_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Filter out citation sentences with wrong intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare citation intent classifier\n",
    "MAX_LEN, BATCH_SIZE = 512, 32\n",
    "PRETRAINED_WEIGHTS=\"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "model = CiteBERT(PRETRAINED_WEIGHTS, D_out=3)\n",
    "state_dict = torch.load(\"../models/CiteSciBERT\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5934/5934 [00:15<00:00, 386.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocess citation sentences\n",
    "dataloader = DataLoader(citation_sentences[\"citation_sentence\"], \n",
    "                        sampler=SequentialSampler(citation_sentences[\"citation_sentence\"]), \n",
    "                        batch_size=BATCH_SIZE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_WEIGHTS)\n",
    "\n",
    "# get input_ids, attention_masks encodings\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for batch in tqdm(dataloader):\n",
    "    b_input_ids, b_attn_mask = tokenizer(batch, \n",
    "                                         max_length=MAX_LEN, \n",
    "                                         padding=\"max_length\",\n",
    "                                         truncation=True,  \n",
    "                                         add_special_tokens=True, \n",
    "                                         return_token_type_ids=False).values()\n",
    "\n",
    "    input_ids.extend(b_input_ids)\n",
    "    attention_masks.extend(b_attn_mask)\n",
    "    \n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for model input\n",
    "encodings = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "# process 1000 sentences each time\n",
    "model_input = DataLoader(encodings[:100], \n",
    "                         sampler=SequentialSampler(encodings[:100]), \n",
    "                         batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 7.77 GiB total capacity; 3.71 GiB already allocated; 17.50 MiB free; 3.86 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-526555084f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# forward pass through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/citation_intent_classification.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         outputs = self.BERT(input_ids=input_ids, \n\u001b[0;32m---> 77\u001b[0;31m                             attention_mask=attention_mask)\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# only take CLS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 )\n\u001b[1;32m    490\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         )\n\u001b[1;32m    409\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/citation/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 7.77 GiB total capacity; 3.71 GiB already allocated; 17.50 MiB free; 3.86 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# pass citation sentences through intent classifier \n",
    "# output index_to_prob (dict)\n",
    "# - index (int): index of a row whose citation sentence has intent \"result-comparison\"\n",
    "# - prob (float): computed probability of \"result-comparison\" intent\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "i, m = 0, 0\n",
    "index_to_prob = {}\n",
    "for batch in tqdm(model_input): \n",
    "        \n",
    "    b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # forward pass through model\n",
    "        logits = model(b_input_ids, b_attn_mask)\n",
    "        \n",
    "    \n",
    "    # compute probabilities of each label\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "    # get indices where \"result-comparison\" has highest probability\n",
    "    ind = np.where(np.argmax(probs, axis=1) == 2)[0]\n",
    "\n",
    "    # create dict of (index, prob) pairs and update index_to_prob\n",
    "    # index = ind + batch_size * i = row number in original input data\n",
    "    d = dict(zip(ind + batch_size * i, probs[ind,2]))\n",
    "    index_to_prob.update(d)\n",
    "        \n",
    "    i += 1\n",
    "    m += len(ind)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
